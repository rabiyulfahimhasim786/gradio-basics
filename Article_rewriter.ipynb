{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMNFBhq7Fzn7wQhhXLRgHZS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rabiyulfahimhasim786/gradio-basics/blob/main/Article_rewriter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Su6TGJKXUs0s",
        "outputId": "5a2f11ce-957e-4684-c7b9-ae1e6e2e0285"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8NQhvZeVZSv",
        "outputId": "fc403116-4cba-45c5-d6e2-5bc5d15f2b79"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from textblob import TextBlob\n",
        "from nltk.corpus import wordnet"
      ],
      "metadata": {
        "id": "GqRajuXkUyuP"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = input(\"Enter the text to be rewritten: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pabn13cJU4k_",
        "outputId": "8b4e5dfc-0c1d-4dda-b64c-736ac555563e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the text to be rewritten: #1 Managed Magento Hosting Provider Lightning-fast managed Magento 2 hosting solution optimized with the latest & powerful stack which includes: Redis Cache, Varnish Cache, MySQL 8.0, Elasticsearch, and more.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = text.lower()  # Convert the text to lowercase\n",
        "blob = TextBlob(text)  # Create a TextBlob object\n",
        "sentences = blob.sentences  # Tokenize the text into sentences"
      ],
      "metadata": {
        "id": "jSKZDgKbU6sg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def synonym_replacement(word, pos=None):\n",
        "    synonyms = []\n",
        "    for syn in wordnet.synsets(word, pos=pos):\n",
        "        for lemma in syn.lemmas():\n",
        "            synonym = lemma.name().replace(\"_\", \" \").lower()\n",
        "            if synonym != word:\n",
        "                synonyms.append(synonym)\n",
        "    if synonyms:\n",
        "        return np.random.choice(synonyms)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def replace_words(sentence):\n",
        "    words = sentence.words\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = synonym_replacement(word)\n",
        "        if new_word is None:\n",
        "            new_words.append(word)\n",
        "        else:\n",
        "            new_words.append(new_word)\n",
        "    return \" \".join(new_words)\n",
        "  \n",
        "rewritten_text = \"\"\n",
        "for sentence in sentences:\n",
        "    new_sentence = replace_words(sentence)\n",
        "    rewritten_text += new_sentence + \" \""
      ],
      "metadata": {
        "id": "ID8AX7PwVH05"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original text:\")\n",
        "print(text)\n",
        "print(\"\\nRewritten text:\")\n",
        "print(rewritten_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNGOYJMQVMo-",
        "outputId": "23ba586b-03d4-4e2f-b4d4-f1d2edb52314"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text:\n",
            "#1 managed magento hosting provider lightning-fast managed magento 2 hosting solution optimized with the latest & powerful stack which includes: redis cache, varnish cache, mysql 8.0, elasticsearch, and more.\n",
            "\n",
            "Rewritten text:\n",
            "ane deal magento host supplier lightning-fast pull off magento ii host resolution optimise with the late mighty heap which include redis squirrel away seal hoard mysql 8.0 elasticsearch and sir thomas more \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GPT2- article rewriter**"
      ],
      "metadata": {
        "id": "XVSPmybOZW-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gpt-2-simple"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDMc7U9HZZ04",
        "outputId": "d052d648-876d-45fa-80e5-031fee3cdbd5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gpt-2-simple\n",
            "  Downloading gpt_2_simple-0.8.1.tar.gz (26 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow>=2.5.1 in /usr/local/lib/python3.8/dist-packages (from gpt-2-simple) (2.11.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from gpt-2-simple) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from gpt-2-simple) (2.25.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from gpt-2-simple) (4.64.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from gpt-2-simple) (1.21.6)\n",
            "Collecting toposort\n",
            "  Downloading toposort-1.9-py3-none-any.whl (8.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.4.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (4.4.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.11.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.6.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (57.4.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.11.2)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (15.0.6.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.14.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.1.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.30.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.4.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.51.1)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (23.1.21)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.2.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.19.6)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.2.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->gpt-2-simple) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->gpt-2-simple) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->gpt-2-simple) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->gpt-2-simple) (2022.12.7)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow>=2.5.1->gpt-2-simple) (0.38.4)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (2.16.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (1.8.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (6.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (3.12.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (3.2.2)\n",
            "Building wheels for collected packages: gpt-2-simple\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpt-2-simple: filename=gpt_2_simple-0.8.1-py3-none-any.whl size=24576 sha256=8d63c46dfefa98efba029da7896efa4b8532d9bbeda0ef5218391cbdf0e91959\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/74/e6/92bfd7a4a0e9358f786896f3d77d6e85ba488b1aff51d1ea2e\n",
            "Successfully built gpt-2-simple\n",
            "Installing collected packages: toposort, gpt-2-simple\n",
            "Successfully installed gpt-2-simple-0.8.1 toposort-1.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gpt_2_simple as gpt2\n",
        "import re"
      ],
      "metadata": {
        "id": "9X83vwPHZoIC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2.download_gpt2(model_name=\"355M\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zafg_c3ZZWgA",
        "outputId": "9cfccbaa-0a06-414d-de22-cd73f2b18159"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 206Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:01, 836kit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 486Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [02:34, 9.18Mit/s]                                 \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 338Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:01, 926kit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 1.20Mit/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, model_name=\"355M\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymBVAFDsZz9r",
        "outputId": "912a91ee-4512-4778-ee5c-396d35d53a57"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained model models/355M/model.ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def article_rewriter(text, num_words):\n",
        "    # Clean input text\n",
        "    text = re.sub(r'\\n+', ' ', text) # Remove new lines\n",
        "    text = re.sub(r'\\s+', ' ', text) # Remove multiple spaces\n",
        "    \n",
        "    # Generate new text\n",
        "    output = gpt2.generate(sess,\n",
        "                           model_name='355M',\n",
        "                           prefix=text,\n",
        "                           length=num_words,\n",
        "                           temperature=0.7,\n",
        "                           nsamples=1,\n",
        "                           batch_size=1,\n",
        "                           return_as_list=True)[0]\n",
        "    \n",
        "    # Clean output text\n",
        "    output = output.replace(text, '')\n",
        "    output = re.sub(r'^[^\\w]+|[^\\w]+$', '', output) # Remove leading/trailing punctuation\n",
        "    output = re.sub(r'\\s+', ' ', output) # Remove multiple spaces\n",
        "    \n",
        "    return output"
      ],
      "metadata": {
        "id": "fWaTzoHAZ4Y7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "An apple is a sweet, edible fruit produced by an apple tree (Malus pumila). Apple trees are cultivated worldwide and are the most widely grown species in the genus Malus. The tree originated in Central Asia, where its wild ancestor, Malus sieversii, is still found today.\n",
        "\"\"\"\n",
        "rewritten_text = article_rewriter(text, num_words=50)\n",
        "print(\"Original text:\\n\", text)\n",
        "print(\"\\nRewritten text:\\n\", rewritten_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDZSvCDLZ7Dy",
        "outputId": "6594c87e-3b09-497c-b127-6ad50bcbda83"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text:\n",
            " \n",
            "An apple is a sweet, edible fruit produced by an apple tree (Malus pumila). Apple trees are cultivated worldwide and are the most widely grown species in the genus Malus. The tree originated in Central Asia, where its wild ancestor, Malus sieversii, is still found today.\n",
            "\n",
            "\n",
            "Rewritten text:\n",
            " It is the most common cultivar of apple trees in North America. There are over 300 wild and cultivated varieties of apple trees.<|endoftext|>New Delhi: The Supreme Court on Thursday directed the government to provide instructions on the development of the state\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "The end of support of Magento 2 is near, and it is the time to choose your Magento store. Whether you want to stay on Magento 1 or to Migrate to M2. However, if you think about moving to Magento 2, there are many Magento 2 requirements that you have to take care of. \n",
        "Without a suitable tech stack and software combination, your eCommerce store could suffer data loss or other migration problems.\n",
        "\"\"\"\n",
        "rewritten_text = article_rewriter(text, num_words=100)\n",
        "print(\"Original text:\\n\", text)\n",
        "print(\"\\nRewritten text:\\n\", rewritten_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uVq0cdyb36B",
        "outputId": "64eb59fe-4911-4d1d-c54f-ef948c1c99ff"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text:\n",
            " \n",
            "The end of support of Magento 2 is near, and it is the time to choose your Magento store. Whether you want to stay on Magento 1 or to Migrate to M2. However, if you think about moving to Magento 2, there are many Magento 2 requirements that you have to take care of. \n",
            "Without a suitable tech stack and software combination, your eCommerce store could suffer data loss or other migration problems.\n",
            "\n",
            "\n",
            "Rewritten text:\n",
            " If you want to migrate to Magento 2, you need to seriously consider the following: 1) The Hosting Platform and Software The hosting platform, software and eCommerce software will have to be updated. If you plan on migrating to Magento 2, you must consider the following: 1) Improve the software and hosting platform. 2) Improve the hosting platform to meet the requirements. 3) Improve the hosting platform to meet the requirements\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "Are you looking to add expiring headers in WordPress? Many website speed testing tools such as Pingdom and GTMetrix would ask you to add expire headers. It is one of the popular WordPress performance optimization suggestions. It can be done easily and the research has shown that it positively affects the speed of the website. But you have to know what webserver your website is using. Let’s see how expire headers help and how you can implement them to your WordPress site. What are Expires Headers and Why Add them? Expires headers are the expiration date assigned to cache files. It hints the web browser whether to download the resource from the web server or to use the one that has been saved in the local browser. The browser saved the website files in the local memory. When the same assets (like website logo is the same for each page) are requested, the browser can fetch the data from the cache. The process is quicker than receiving the files from the server. Consequently it improved the speed of the website and reduced the bandwidth usage. KeyCDN image But the browser should know if it is fine to get the saved file or if it is time to get the new version. The expires headers set the time to use fresh content You can set the different time for the different types of data file. For the text it could be 3 months, as the text is regularly changed. While the images are not changed much so that they could be cached for 6 months. How to fix the “Expires Headers” in WordPress? There are three ways to fix ‘Adjust Expires Headers’ in WordPress. Let’s see. #1 Add Expires Headers with a WordPress plugin - Like everything else, there is a WordPress plugin to fix Add Expires Headers. Expires headers search in the repository of the WordPress plugins. The plugin Expires Headers definitely works but its functionality is limited to this task. But as a dedicated plugin, it contains multiple options related to adding expires headers. For example, you can select the different expiry time for different media files. If you are already using WP Fastest Cache, W3 Cache, WP Rocket and WP Super Cache, find the expires header in those plugins. #2 Add Expires Headers with.htaccess on Apache Web Server If your WordPress website hosts an Apache web server, you can add Expires headers by adding code into the.htaccess file. The.htaccess file is in the root folder of the server. If you have FTP access, follow these steps: Connect to the website server using FTP credentials Find the.htaccess file in the root folder Download the original file as a backup ## EXPIRES HEADER CACHING ## IfModule mod_expires.c> Expiresactive on ExpiresByType image/jpeg \"access 1 year\" ExpiresByType application/pdf \"access 1 month\" ExpiresBy If you don’t have FTP access, you need to go to the Control Panel/Administration Panel to edit the file. Login to your web hosting Locate the cPanel and open it Open the File Manager Go to the WordPress Root folder Download the.htaccess file for backup Edit the file Save the changes #3 Insert Headers with config file on Nginx Web Server if your WordPress hosting is using Nginx server you have to add the code to the server configuration file. To add the code to the server, you must contact the hosting team. At ServerGuy we do it automatically to every website we host. The code is  location . (jpg|jpeg|gif|png|svg)$  expires 365d;  location *  (pdf|css|html|js|swf)$ expires 2d;  Test if expires headers are working Once the expires headers are added to WordPress, you should check if it works or not. Open the giftofspeed tool Test your URL and the application will list all the resources and their expiry time. Another method is to use the curl command. Connect to the server and open the terminal inside the root file Run the following command on a website asset. curl-I https://serverguy.com/wp-content/plugins/betterdocs/public/css/betterdocs-public.css The output shows the URL information – which includes the expires headers. Final words Adding expires headers is great for website speed. It helps you to manage the file caches of different file formats on your site. Speed Optimization tools by default decrease the speed if headers expire and your client might complain. It would be better to do it. However, if you are on ServerGuy WordPress Hosting, you should not worry about it. We do it automatically and assist the website owners in personalizing the timing as they like. In this quick tutorial we learnt how to add ends headers. If you encounter any issue, please feel free to leave it in the comment section. \n",
        "\"\"\"\n",
        "rewritten_text = article_rewriter(text, num_words=100)\n",
        "print(\"Original text:\\n\", text)\n",
        "print(\"\\nRewritten text:\\n\", rewritten_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uL4vznQOh9aI",
        "outputId": "a37d25e6-8cff-459f-aef3-80f4a0ebed07"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text:\n",
            " \n",
            "Are you looking to add expiring headers in WordPress? Many website speed testing tools such as Pingdom and GTMetrix would ask you to add expire headers. It is one of the popular WordPress performance optimization suggestions. It can be done easily and the research has shown that it positively affects the speed of the website. But you have to know what webserver your website is using. Let’s see how expire headers help and how you can implement them to your WordPress site. What are Expires Headers and Why Add them? Expires headers are the expiration date assigned to cache files. It hints the web browser whether to download the resource from the web server or to use the one that has been saved in the local browser. The browser saved the website files in the local memory. When the same assets (like website logo is the same for each page) are requested, the browser can fetch the data from the cache. The process is quicker than receiving the files from the server. Consequently it improved the speed of the website and reduced the bandwidth usage. KeyCDN image But the browser should know if it is fine to get the saved file or if it is time to get the new version. The expires headers set the time to use fresh content You can set the different time for the different types of data file. For the text it could be 3 months, as the text is regularly changed. While the images are not changed much so that they could be cached for 6 months. How to fix the “Expires Headers” in WordPress? There are three ways to fix ‘Adjust Expires Headers’ in WordPress. Let’s see. #1 Add Expires Headers with a WordPress plugin - Like everything else, there is a WordPress plugin to fix Add Expires Headers. Expires headers search in the repository of the WordPress plugins. The plugin Expires Headers definitely works but its functionality is limited to this task. But as a dedicated plugin, it contains multiple options related to adding expires headers. For example, you can select the different expiry time for different media files. If you are already using WP Fastest Cache, W3 Cache, WP Rocket and WP Super Cache, find the expires header in those plugins. #2 Add Expires Headers with.htaccess on Apache Web Server If your WordPress website hosts an Apache web server, you can add Expires headers by adding code into the.htaccess file. The.htaccess file is in the root folder of the server. If you have FTP access, follow these steps: Connect to the website server using FTP credentials Find the.htaccess file in the root folder Download the original file as a backup ## EXPIRES HEADER CACHING ## IfModule mod_expires.c> Expiresactive on ExpiresByType image/jpeg \"access 1 year\" ExpiresByType application/pdf \"access 1 month\" ExpiresBy If you don’t have FTP access, you need to go to the Control Panel/Administration Panel to edit the file. Login to your web hosting Locate the cPanel and open it Open the File Manager Go to the WordPress Root folder Download the.htaccess file for backup Edit the file Save the changes #3 Insert Headers with config file on Nginx Web Server if your WordPress hosting is using Nginx server you have to add the code to the server configuration file. To add the code to the server, you must contact the hosting team. At ServerGuy we do it automatically to every website we host. The code is  location . (jpg|jpeg|gif|png|svg)$  expires 365d;  location *  (pdf|css|html|js|swf)$ expires 2d;  Test if expires headers are working Once the expires headers are added to WordPress, you should check if it works or not. Open the giftofspeed tool Test your URL and the application will list all the resources and their expiry time. Another method is to use the curl command. Connect to the server and open the terminal inside the root file Run the following command on a website asset. curl-I https://serverguy.com/wp-content/plugins/betterdocs/public/css/betterdocs-public.css The output shows the URL information – which includes the expires headers. Final words Adding expires headers is great for website speed. It helps you to manage the file caches of different file formats on your site. Speed Optimization tools by default decrease the speed if headers expire and your client might complain. It would be better to do it. However, if you are on ServerGuy WordPress Hosting, you should not worry about it. We do it automatically and assist the website owners in personalizing the timing as they like. In this quick tutorial we learnt how to add ends headers. If you encounter any issue, please feel free to leave it in the comment section. \n",
            "\n",
            "\n",
            "Rewritten text:\n",
            " _______________________________________________________ WordPress Technical Team - @wordpressteam<|endoftext|>What is this? It's a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Alternative paraphrasing**"
      ],
      "metadata": {
        "id": "VzWMrVTRjdFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install spacy\n",
        "!pip install textblob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41RExFWpjfxY",
        "outputId": "023554d5-4b67-420a-a096-bb3f12d6161a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (3.4.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (8.1.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (23.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.10.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.25.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.8/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.8/dist-packages (from textblob) (3.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk>=3.1->textblob) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk>=3.1->textblob) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk>=3.1->textblob) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk>=3.1->textblob) (1.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import spacy\n",
        "from textblob import TextBlob\n",
        "from nltk.corpus import wordnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipqj1CIajilI",
        "outputId": "12c02b6b-2598-48b9-d328-9225cbd42449"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEthGdaXkFla",
        "outputId": "40e6c0a5-6db4-404d-ba54-4c6b60a3cddd"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "wCo2gQyXj77S"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def article_rephraser(text):\n",
        "    # Tokenize the input text\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.text for token in doc]\n",
        "    \n",
        "    # Perform part-of-speech tagging\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "    \n",
        "    # Replace synonyms of nouns and adjectives\n",
        "    rephrased_tokens = []\n",
        "    for word, tag in pos_tags:\n",
        "        if tag.startswith('NN'):\n",
        "            synsets = wordnet.synsets(word, pos=wordnet.NOUN)\n",
        "            if synsets:\n",
        "                synonym = synsets[0].lemmas()[0].name()\n",
        "                rephrased_tokens.append(synonym.replace('_', ' '))\n",
        "            else:\n",
        "                rephrased_tokens.append(word)\n",
        "        elif tag.startswith('JJ'):\n",
        "            synsets = wordnet.synsets(word, pos=wordnet.ADJ)\n",
        "            if synsets:\n",
        "                synonym = synsets[0].lemmas()[0].name()\n",
        "                rephrased_tokens.append(synonym.replace('_', ' '))\n",
        "            else:\n",
        "                rephrased_tokens.append(word)\n",
        "        else:\n",
        "            rephrased_tokens.append(word)\n",
        "    \n",
        "    # Convert rephrased tokens to text\n",
        "    rephrased_text = ' '.join(rephrased_tokens)\n",
        "    \n",
        "    # Use TextBlob to correct spelling and grammar\n",
        "    rephrased_text = TextBlob(rephrased_text).correct()\n",
        "    \n",
        "    return rephrased_text"
      ],
      "metadata": {
        "id": "4RUjCFv-j-kj"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "An apple is a sweet, edible fruit produced by an apple tree (Malus pumila). Apple trees are cultivated worldwide and are the most widely grown species in the genus Malus. The tree originated in Central Asia, where its wild ancestor, Malus sieversii, is still found today.\n",
        "\"\"\"\n",
        "rephrased_text = article_rephraser(text)\n",
        "print(\"Original text:\\n\", text)\n",
        "print(\"\\nRephrased text:\\n\", rephrased_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtUdayBykA4Y",
        "outputId": "0678ab81-b617-45a1-fb44-bb76e18e9be2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text:\n",
            " \n",
            "An apple is a sweet, edible fruit produced by an apple tree (Malus pumila). Apple trees are cultivated worldwide and are the most widely grown species in the genus Malus. The tree originated in Central Asia, where its wild ancestor, Malus sieversii, is still found today.\n",
            "\n",
            "\n",
            "Rephrased text:\n",
            " \n",
            " In apple is a sweet , enable fruit produced by an apple tree ( Value pupil ) . apple tree are cultivated worldwide and are the most widely adult species in the genus Value . The tree originated in central Asia , where its wild ancestor , Value sieversii , is still found today . \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "The end of support of Magento 2 is near, and it is the time to choose your Magento store. Whether you want to stay on Magento 1 or to Migrate to M2. However, if you think about moving to Magento 2, there are many Magento 2 requirements that you have to take care of. \n",
        "Without a suitable tech stack and software combination, your eCommerce store could suffer data loss or other migration problems. \n",
        "\"\"\"\n",
        "rephrased_text = article_rephraser(text)\n",
        "print(\"Original text:\\n\", text)\n",
        "print(\"\\nRephrased text:\\n\", rephrased_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mg-U7bzxl3Gk",
        "outputId": "7efbca0f-1ec3-42e2-e938-762033c71664"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text:\n",
            " \n",
            "The end of support of Magento 2 is near, and it is the time to choose your Magento store. Whether you want to stay on Magento 1 or to Migrate to M2. However, if you think about moving to Magento 2, there are many Magento 2 requirements that you have to take care of. \n",
            "Without a suitable tech stack and software combination, your eCommerce store could suffer data loss or other migration problems. \n",
            "\n",
            "\n",
            "Rephrased text:\n",
            " \n",
            " The end of support of Agents 2 is near , and it is the time to choose your Agents shop . Whether you want to stay on Agents 1 or to Migrate to Of . However , if you think about moving to Agents 2 , there are many Agents 2 requirement that you have to take care of . \n",
            " Without a suitable technical school stick and software combination , your commerce shop could suffer data loss or other migration problem . \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**solution1**"
      ],
      "metadata": {
        "id": "_CEpObLa2HHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yq08ASI82Gp9",
        "outputId": "98e91813-0898-4edf-82b5-f629e22c29c7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import random\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Read the input article\n",
        "with open('input.txt', 'r') as file:\n",
        "    article = file.read()\n",
        "\n",
        "# Tokenize the input article\n",
        "tokens = word_tokenize(article)\n",
        "\n",
        "# Create a list of synonyms\n",
        "synonyms = []\n",
        "for token in tokens:\n",
        "    syns = wordnet.synsets(token)\n",
        "    for syn in syns:\n",
        "        for lemma in syn.lemmas():\n",
        "            synonyms.append(lemma.name())\n",
        "\n",
        "# Replace words with synonyms\n",
        "output = ''\n",
        "for token in tokens:\n",
        "    if re.match(r'\\W+', token):\n",
        "        output += token\n",
        "    else:\n",
        "        syns = wordnet.synsets(token)\n",
        "        if syns:\n",
        "            synonym = random.choice(synonyms)\n",
        "            output += synonym\n",
        "        else:\n",
        "            output += token\n",
        "    output += ' '\n",
        "\n",
        "# Write the output to a file\n",
        "with open('output.txt', 'w') as file:\n",
        "    file.write(output)"
      ],
      "metadata": {
        "id": "lPf9fz552MIo"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**solution2**"
      ],
      "metadata": {
        "id": "4l4TNZPs2P32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import random\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Read the input article\n",
        "with open('input.txt', 'r') as file:\n",
        "    article = file.read()\n",
        "\n",
        "# Tokenize the input article\n",
        "tokens = word_tokenize(article)\n",
        "\n",
        "# Create a list of synonyms\n",
        "synonyms = []\n",
        "for token in tokens:\n",
        "    syns = wordnet.synsets(token)\n",
        "    for syn in syns:\n",
        "        for lemma in syn.lemmas():\n",
        "            synonyms.append(lemma.name())\n",
        "\n",
        "# Replace words with synonyms\n",
        "output = ''\n",
        "for token in tokens:\n",
        "    if re.match(r'\\W+', token):\n",
        "        output += token\n",
        "    else:\n",
        "        syns = wordnet.synsets(token)\n",
        "        if syns:\n",
        "            synonym = random.choice(synonyms)\n",
        "            output += synonym\n",
        "        else:\n",
        "            output += token\n",
        "    output += ' '\n",
        "\n",
        "# Write the output to a file\n",
        "with open('output.txt', 'w') as file:\n",
        "    file.write(output)"
      ],
      "metadata": {
        "id": "qqEx_sWh2PmK"
      },
      "execution_count": 34,
      "outputs": []
    }
  ]
}